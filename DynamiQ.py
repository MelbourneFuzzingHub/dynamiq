import sys
import argparse
import os
import signal
import time
import subprocess
import psutil
from shutil import copyfile
import logging
import shutil
import hashlib
import uuid

# Import DynamiQ's modules
from utils import helpers
from utils import extractors
from graphs import networkx_processing as nxp
from tasks import ldg_partitioning as ldg
from tasks import fennel_partitioning as fennel
from tasks import hrdf_partitioning as hrdf
from tasks import random_partitioning as random_part

'''
Main loop of DynamiQ
'''

def main(binary_name, afl_binary, profiling_binary, gcov_binary, gcov_folders, pre_args, post_args,
         seed_corpus, out_folder, dict_file, dot_file, func_ids, func_bbs, cores, algorithm,
         total_timeout, scanning_timeout, expl_timeout, target_folder, libafl_dir=None, cmplog_binary=None, use_cmplog_in_task=False):
    # Create a log file
    LOG_FILENAME = os.path.join(out_folder, "DynamiQ.log")
    logging.basicConfig(filename=LOG_FILENAME, level=logging.DEBUG)
    logging.info("[Initialization] DynamiQ starts")

     # Check if profiling_binary and gcov_binary exist
    if not os.path.exists(profiling_binary):
        logging.warning(f"Warning: Profiling binary '{profiling_binary}' does not exist. Proceeding anyway.")
    
    if not os.path.exists(gcov_binary):
        logging.warning(f"Warning: GCOV binary '{gcov_binary}' does not exist. Proceeding anyway.")

    # Remove gcov files if they exist
    nxp.remove_gcov_files(gcov_folders)

    # Get the initial callgraph & related information
    # v_fname_dict: a dictionary to map from vertices to function names
    # fname_v_dict: a dictionary to map from function names to vertices
    # main_v: the root vertex
    (CG, v_fname_dict, fname_v_dict, main_v) = nxp.extract_callgraph(dot_file)

    # Get all functions and their corresponding source files based on
    # the func_ids.log file generated by HF_BINARY afl-clang-fast
    fname_src_dict = extractors.extract_fname_src_dict(func_ids)

    # Get all functions and their corresponding basic block count
    fname_bbs_dict = extractors.extract_fname_bbs_dict(func_bbs)

    # Start the monitoring fuzzer which is always running
    # Prepare necessary directories
    activeDir = os.path.join(out_folder, "active_runs")
    instanceDir = os.path.join(out_folder, "instances")
    originalSeedDir = os.path.join(activeDir, "seeds_origin")
    seedDir = os.path.join(activeDir, "seeds")
    taskDir = os.path.join(activeDir, "tasks")
    shared_sync_dir = os.path.join(activeDir, "shared_sync")

    # Create them all in a loop
    for d in [activeDir, instanceDir, originalSeedDir, seedDir, taskDir, shared_sync_dir]:
        os.makedirs(d, exist_ok=True)

    # Copy seed corpus to original seed directory
    for seed in os.listdir(seed_corpus):
        seed_path = os.path.join(seed_corpus, seed)
        if os.path.isfile(seed_path):
            copyfile(seed_path, os.path.join(originalSeedDir, seed))

    # Base fuzzing options
    common_fuzzing_options = ["-m", "2G", "-t", "20000+", "-o", activeDir]
    if dict_file:  # Include dictionary if provided
        common_fuzzing_options += ["-x", dict_file]
    # Define the specific fuzzing options
    team_name = f"DynamiQ-{binary_name}"
    specific_fuzzing_options = common_fuzzing_options + ["-i", originalSeedDir, "-M", "monitor", "-T", team_name]

    # Generate the -F options for each fuzzer_x from 1 to (cores - 1)
    for i in range(1, cores):
        fuzzer_queue = os.path.join(instanceDir, f"fuzzer_{i}/queue")
        specific_fuzzing_options += ["-F", fuzzer_queue]

    # Appeng the shared sync directory to the monitor command
    specific_fuzzing_options += ["-F", shared_sync_dir]

   # Build the fuzzing command with cmplog binary if specified
    fuzz_command = ["afl-fuzz"] + specific_fuzzing_options
    if cmplog_binary:
        fuzz_command += ["-c", cmplog_binary]
    
    fuzz_command += [afl_binary]

    # Add pre-arguments if any
    if pre_args.strip():
        fuzz_command += pre_args.strip().split()

    # Add the input placeholder
    fuzz_command += ["@@"]

    # Add post-arguments if any
    if post_args.strip():
        fuzz_command += post_args.strip().split()

    # Set environment variables
    env = os.environ.copy()
    env['AFL_SYNC_TIME'] = '15'
    env['AFL_FINAL_SYNC'] = '1'
    interrupted = False  # Flag to detect manual interruption

    # Execute the command
    logging.info("[Initialization] Monitor command: %s", ' '.join(fuzz_command))

    # Get an available core
    try:
        monitor_core = helpers.get_next_available_core()
    except RuntimeError as e:
        logging.error("Could not find an available core: %s", e)
        sys.exit(1)

    # Execute the monitor command on the selected core
    env = os.environ.copy()
    # Start the process without specifying cpu_affinity initially
    pmonitor = subprocess.Popen(
        fuzz_command,
        env=env
    )

    # Set affinity to the desired core
    psutil.Process(pmonitor.pid).cpu_affinity([monitor_core])

    # Small delay to allow the OS to finalize any core assignments
    time.sleep(0.1)

    # Re-check the actual affinity, either with psutil or taskset as an external command
    actual_core_affinity = psutil.Process(pmonitor.pid).cpu_affinity()

    # Alternatively, using taskset to verify:
    taskset_check = subprocess.check_output(f"taskset -pc {pmonitor.pid}", shell=True).decode().strip()

    logging.info("[Initialization] Monitor PID: %d, Requested Core: %d, Psutil Core Affinity: %s, Taskset Check: %s",
                pmonitor.pid, monitor_core, actual_core_affinity, taskset_check)

    all_deleted_pairs = set()

    # Create map size
    map_size = "65536"

    # Whether to use llvm coverage
    use_llvm = False
    use_unit_diameter = False
    if binary_name=="ossfuzz":
        use_llvm = True
        use_unit_diameter = True
        # map_size = "262144"
        # logging.info(f"Setting AFL_MAP_SIZE to {map_size}")

    # Create the combined command-line arguments
    cmdline = []
    if pre_args and pre_args.strip():
        cmdline.extend(pre_args.strip().split())
    cmdline.append("@@")
    if post_args and post_args.strip():
        cmdline.extend(post_args.strip().split())
    cmdline_str = ' '.join(cmdline)  # Convert to string for logging/debugging

    # Log the constructed command line
    # logging.info("[Initialization] Constructed Command Line: {cmdline_str}")

    # Generate an 8-char hash as dir name
    raw_hash = hashlib.sha1(str(uuid.uuid4()).encode()).hexdigest()[:8]
    logging.info(f"[Initialization] Generated unique hash for this run: {raw_hash}")
    # This is for running expriments in parallel, otherwise using binary_name should be enough
    extracted_binary_name = os.path.basename(os.path.dirname(func_ids))
    logging.info("[Initialization] Extracted binary name: %s", extracted_binary_name)
    helpers.deduplicate_tmp_logs(extracted_binary_name)

    # Record the start time of the entire process
    start_time = time.time()

    # Start other fuzzing instances
    exploitMode = False  # Start in exploration mode
    curRound = 1
    while True:
        logging.info("-------------------------------------------------------------------------------------------------------------------")
        logging.debug(f"[round {curRound}] Starting DynamiQ round with exploitMode={exploitMode}")

        # Step-0. Save results from the previous round & clean results
        if curRound > 1:
            backupDir = os.path.join(out_folder, f"backup_round_{curRound - 1}")
            os.makedirs(backupDir, exist_ok=True)

            # Save the defined tasks for analysis/debugging
            if os.path.exists(os.path.join(activeDir, "tasks")):
                shutil.move(os.path.join(activeDir, "tasks"), backupDir)

            # Sync seeds from all fuzzer instances into shared_sync
            if curRound == 2:
                src_dirs = [os.path.join(instanceDir, "fuzzer_1", "queue")]
            else:
                src_dirs = [os.path.join(instanceDir, f"fuzzer_{i}", "queue") for i in range(1, cores)]
            helpers.sync_unique_testcases(src_dirs, shared_sync_dir)

            # Save intermediate output log
            if os.path.exists(os.path.join(instanceDir, "output.txt")):
                shutil.move(os.path.join(instanceDir, "output.txt"), backupDir)
            if os.path.exists(os.path.join(instanceDir, "cpu_bindings.txt")):
                shutil.move(os.path.join(instanceDir, "cpu_bindings.txt"), backupDir)
        # Step-1. Prepare seed corpus
        seed_corpus_path = os.path.join(activeDir, "monitor", "queue")
        if curRound == 1:
            seed_corpus_path = originalSeedDir
            seed_corpus_path_sync = os.path.join(activeDir, "monitor", "queue")

        helpers.count_seeds_in_seed_dir(seed_corpus_path, curRound)

        if curRound == 1:
            logging.debug("[round 1]  Skip updating the callgraph in round 1")

        # Step-2. Do task generation in exploitation mode
        else:
            exploitMode = True  # Set exploitation mode to True after the first round

            # Update the callgraph
            logging.debug("[CallGraph] Nodes and Edges before update in round %d: %d, %d", curRound, len(CG.nodes), len(CG.edges))
            added_pairs = nxp.update_callgraph(extracted_binary_name, pre_args, post_args, CG, v_fname_dict, fname_v_dict, fname_src_dict,
                                 profiling_binary, gcov_binary, gcov_folders,
                                 os.path.join(activeDir, "monitor", "queue"), False, use_llvm) # true if use llvm coverage
            logging.debug("[CallGraph] Nodes and Edges after update in round %d: %d, %d", curRound, len(CG.nodes), len(CG.edges))
            logging.debug("[CallGraph] Added pairs after update: %d", len(added_pairs))
            # logging.debug("Is CG still acyclic?: %s", nx.is_directed_acyclic_graph(CG))

            # For HRDF: Use prune_level = 3 → Skip first and third phases.
            # Reason: - HRDF partitioner does NOT assign isolated nodes (no edges), so we must retain them in the graph.
            prune_level = 3 if algorithm == "hrdf" else 0  # Others run in diagnostic mode (no pruning)

            deleted_pairs = nxp.prune_callgraph(CG, main_v, v_fname_dict, fname_v_dict,
                                                fname_bbs_dict, fname_src_dict, prune_level)
            # Update global deleted pair set
            all_deleted_pairs.update(deleted_pairs)
            # Only subtract those added pairs that were not deleted again
            # logging.debug("All added pairs that were not deleted: %d", len(added_pairs - deleted_pairs))
            all_deleted_pairs.difference_update(added_pairs - deleted_pairs)
            logging.debug("[CallGraph] All deleted pairs after pruning: %d", len(all_deleted_pairs))

            # Create a directory to store all tasks if necessary
            if not os.path.exists(taskDir):
                os.mkdir(taskDir)

            logging.info("[Partitioning] Starting task generation with algorithm: %s", algorithm)
            # Proceed with the normal task generation process
            partitioners = {
                "ldg": ldg.LDGPartitioner,
                "fennel": fennel.FennelPartitioner,
                "hrdf": hrdf.HDRFPartitioner,
                "random": random_part.RandomPartitioner,
            }

            if algorithm in partitioners:
                partitioner = partitioners[algorithm]()
                partitioner.partition(
                    CG, main_v, v_fname_dict, fname_src_dict, fname_bbs_dict,
                    cores - 1, taskDir, all_deleted_pairs, None, use_unit_diameter
                )
            else:
                raise ValueError(f"Unsupported partitioning algorithm: {algorithm}")

        # Step-3. Start fuzzing instances
        popens = []
        logging.debug(f"[round {curRound}] Starting fuzzing instances")
        if exploitMode:
            # Navigate to the libafl fuzzer directory
            os.chdir(libafl_dir)

            # Start fuzzing instances in exploitation mode
            rust_command = [
                "cargo", "run", "--release", "--bin", "run", "--",
                "--corpus-dir", seed_corpus_path,
                "--active-dir", instanceDir,
                "--task-dir", taskDir,
                "--target-dir", target_folder,
                "--binary-name", binary_name,
                "--num-cores", str(cores - 1),
                "--output-folder", out_folder,
                "--sync-instrumentation" # Enable instrumentaion in parallel
            ]
            # Indicate whether it is the first round for exploitation
            if curRound == 2:
                rust_command += ["--first-run"]
            # Add optional dictionary if provided
            if dict_file:
                rust_command += ["--token-file", dict_file]
            if map_size != "65536":
                rust_command += ["--map-size", map_size]
            if raw_hash:
                rust_command += ["--hash", raw_hash]
            if cmplog_binary and use_cmplog_in_task:
                rust_command += ["--cmplog-binary", cmplog_binary]
            # Add the command line last
            rust_command += ["--cmdline", cmdline_str]
            logging.debug(f"[round {curRound}] Exploitation rust command: {' '.join(rust_command)}")

            # Define output file for Rust fuzzer logs
            output_file_path = os.path.join(instanceDir, "cpu_bindings.txt")
            # Open the output file to redirect stdout and stderr
            with open(output_file_path, "w") as output_file:
                rust_proc = subprocess.Popen(
                    rust_command,
                    stdout=output_file,
                    stderr=output_file,
                    preexec_fn=os.setsid
                )
                popens.append(rust_proc)
            os.chdir('..')

        elif exploitMode == False and scanning_timeout > 0:
            # Navigate to the libafl fuzzer directory
            os.chdir(libafl_dir)
            logging.debug(f"[round {curRound}] Seed corpus: %s", seed_corpus_path)

            # Start fuzzing instances in exploration mode
            rust_command = [
                "cargo", "run", "--release", "--bin", "parallel_sync_cmplog" if cmplog_binary and use_cmplog_in_task else "parallel_sync", "--",
                "--core-num", str(cores - 1),
                "--initial-corpus", os.path.join(instanceDir, "fuzzer_1", "queue"),
                "--corpus-dir", seed_corpus_path,
                "--binary-path", afl_binary,
                "--output-folder", out_folder,
                "--foreign-sync-dirs", seed_corpus_path_sync
            ]

            if actual_core_affinity:
                # join into comma-separated list
                skip = ",".join(str(c) for c in actual_core_affinity)
                rust_command += ["--skip-cores", skip]
            # Add optional dictionary if provided
            if dict_file:
                rust_command += ["--token-file", dict_file]
            if map_size != "65536":
                rust_command += ["--map-size", map_size]
            # Add optional cmplog binary
            if cmplog_binary and use_cmplog_in_task:
                rust_command += ["--cmplog-dir", cmplog_binary]
            rust_command += ["--cmdline", cmdline_str]
            logging.debug(f"[round {curRound}] Exploration rust command: {' '.join(rust_command)}")
            output_file_path = os.path.join(out_folder, "parallel_output.txt")
            with open(output_file_path, "w") as output_file:
                rust_proc = subprocess.Popen(
                    rust_command, 
                    stdout=output_file, 
                    stderr=output_file, 
                    preexec_fn=os.setsid
                )
            os.chdir('..')
            popens.append(rust_proc)
        else:
            logging.debug(f"[round {curRound}] Skipping exploration phase (scanning_timeout = 0)")

        # Step-4. Wait until the stopping criteria meet
        current_time = time.time()
        elapsed_time = current_time - start_time
        time_left = total_timeout - elapsed_time

        try:
            if exploitMode:
                # Adjust the exploitation timeout if necessary
                adjusted_expl_timeout = min(expl_timeout, max(0, time_left))
                logging.debug(f"[round {curRound}] Exploitation mode: waiting for {adjusted_expl_timeout} seconds")
                helpers.should_stop_timeout(adjusted_expl_timeout)
            else:
                # Adjust the scanning timeout if necessary
                adjusted_scanning_timeout = min(scanning_timeout, max(0, time_left))
                logging.debug(f"[round {curRound}] Scanning mode: waiting for {adjusted_scanning_timeout} seconds")
                helpers.should_stop_timeout(adjusted_scanning_timeout)
        except KeyboardInterrupt:
            logging.debug(f"[round {curRound}] Fuzzing interrupted manually; preparing to shut down after cleanup.")
            interrupted = True  # Mark for post-cleanup exit

        # Update elapsed time after waiting
        current_time = time.time()
        elapsed_time = current_time - start_time
        logging.debug(f"[round {curRound}] Elapsed time: {elapsed_time} seconds")

        # After waiting, read the PIDs from the file if it exists
        pid_file = os.path.join(out_folder, "pids.txt")
        if os.path.exists(pid_file):
            with open(pid_file, 'r') as f:
                pids = [int(line.strip()) for line in f.readlines() if line.strip()]

            for pid in pids:
                try:
                    proc = psutil.Process(pid)
                    popens.append(proc)
                    # logging.debug("Captured PID: %d", pid)
                except psutil.NoSuchProcess:
                    logging.debug(f"[round {curRound}] Process with PID {pid} does not exist")

        # Send SIGTERM signal to stop fuzzing instances
        for p in popens:
            try:
                # Wrap Popen objects with psutil.Process to check process status
                proc = psutil.Process(p.pid)
                if proc.is_running():
                    os.killpg(os.getpgid(proc.pid), signal.SIGTERM)
                    p.wait()  # Wait for the process to terminate
                    logging.debug(f"[round {curRound}] Process {proc.pid} terminated successfully.")
                else:
                    logging.debug(f"[round {curRound}] Process {proc.pid} is not running and does not need termination.")
            except psutil.NoSuchProcess:
                pass
                # logging.debug(f"Process {p.pid} does not exist or is already terminated.")
            except Exception as e:
                logging.error(f"[round {curRound}] Failed to terminate process {p.pid}: {e}")


        # Delete the pids.txt file after stopping the processes
        if os.path.exists(pid_file):
            os.remove(pid_file)
            logging.debug(f"[round {curRound}] Deleted PID file: {pid_file}")

        # Finalize round
        if interrupted:
            logging.debug(f"[round {curRound}] Cleanup complete after interruption, exiting.")
            break  # Exit main loop after cleanup

        # Step-5. Finalize current round and prepare for next round
        curRound += 1

        # Break the loop if total_timeout is reached
        if elapsed_time >= total_timeout:
            logging.debug(f"[round {curRound}] Total timeout reached, ending fuzzing")
            break

    # Copy final seeds to the seed directory before terminating
    helpers.count_seeds_in_seed_dir(os.path.join(activeDir, "monitor", "queue"), curRound, copy_seeds=True, seedDir=seedDir)

    # Terminate the monitor process
    os.kill(pmonitor.pid, signal.SIGTERM)
    pmonitor.wait()

    logging.debug(f"[round {curRound}] DynamiQ ends!!!")
    return 0

# Parse the input arguments
if __name__ == '__main__':
    print(
        r"""
                                                                                                           
        ██████╗ ██╗   ██╗███╗   ██╗ █████╗ ███╗   ███╗██╗ ██████╗ 
        ██╔══██╗╚██╗ ██╔╝████╗  ██║██╔══██╗████╗ ████║██║██╔═══██╗
        ██║  ██║ ╚████╔╝ ██╔██╗ ██║███████║██╔████╔██║██║██║   ██║
        ██║  ██║  ╚██╔╝  ██║╚██╗██║██╔══██║██║╚██╔╝██║██║██║▄▄ ██║
        ██████╔╝   ██║   ██║ ╚████║██║  ██║██║ ╚═╝ ██║██║╚██████╔╝
        ╚═════╝    ╚═╝   ╚═╝  ╚═══╝╚═╝  ╚═╝╚═╝     ╚═╝╚═╝ ╚══▀▀═╝ 
                                                                                                        
                                                                                                                                                            
        DynamiQ - Dynamic Task Allocation in Parallel Fuzzing
        """
    )
    parser = argparse.ArgumentParser()
    parser.add_argument('-bn', '--binary', type=str, required=True,
                        help="Binary name (e.g., libpng_read_fuzzer)")
    parser.add_argument('-ab', '--afl_binary', type=str, required=True,
                        help="Full path to the afl-instrumented binary")
    parser.add_argument('-pb', '--profiling_binary', type=str, required=True,
                        help="Full path to the profiling binary")
    parser.add_argument('-gb', '--gcov_binary', type=str, required=True,
                        help="Full path to the gcov binary")
    parser.add_argument('-gf', '--gcov_folders', type=str, required=True, nargs='+',
                        help="Full paths to the gcov folders, separated by spaces")
    parser.add_argument('-ea1', '--pre_arguments', type=str, required=True,
                        help="Extra argument(s) to run the binary that needs to be in front of @@")
    parser.add_argument('-ea2', '--post_arguments', type=str, required=True,
                        help="Extra argument(s) to run the binary that needs to be behind @@")
    parser.add_argument('-i', '--seed_corpus', type=str, required=True,
                        help="Full path to the seed corpus")
    parser.add_argument('-o', '--out_folder', type=str, required=True,
                        help="Full path to the output folder keeping all results")
    parser.add_argument('-x', '--dict', type=str, required=False,
                        help="Full path to the dictionary file (optional)")
    parser.add_argument('-d', '--dot_file', type=str, required=True,
                        help="Full path to dot file generated by LLVM opt")
    parser.add_argument('-c', '--cores', type=int, required=True,
                        help="Number of CPU cores")
    parser.add_argument('-a', '--algorithm', type=str, required=True, choices=["ldg", "fennel", "hrdf", "random"],
                        help="Partitioning algorithm (e.g., ldg, fennel, hrdf, random)")
    parser.add_argument('-tt', '--total_timeout', type=int, required=True,
                        help="Timeout in seconds for the whole experiment")
    parser.add_argument('-st', '--scanning_timeout', type=int, required=True,
                        help="Timeout in seconds for scanning/exploration phase")
    parser.add_argument('-et', '--expl_timeout', type=int, required=True,
                        help="Timeout in seconds for exploitation phase")
    parser.add_argument('-tf', '--target_folder', type=str, required=True,
                        help="Full path to the target directory for applying selective instrumentation")
    parser.add_argument('-ld', '--libafl_dir', type=str, required=False, help=argparse.SUPPRESS)
    parser.add_argument('-cb', '--cmplog_binary', type=str, help="Optional cmplog binary")
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--use_cmplog_in_task', dest='use_cmplog_in_task', action='store_true', help=argparse.SUPPRESS)
    group.add_argument('--no-use_cmplog_in_task', dest='use_cmplog_in_task', action='store_false', help=argparse.SUPPRESS)
    parser.set_defaults(use_cmplog_in_task=None)  # default is "unspecified"
    parser.add_argument('-f', '--func_ids', type=str, required=False, help=argparse.SUPPRESS)
    parser.add_argument('-b', '--func_bbs', type=str, required=False, help=argparse.SUPPRESS)
    args = parser.parse_args()
    # Resolve libafl_dir to the default (fuzzers/ under current script directory) if not provided
    if args.libafl_dir:
        libafl_dir = args.libafl_dir
    else:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        libafl_dir = os.path.join(script_dir, "fuzzers")
        print(f"[+] Using default libafl_dir: {libafl_dir}")
    if args.func_ids and args.func_bbs:
        func_ids = args.func_ids
        func_bbs = args.func_bbs
    else:
        func_ids, func_bbs = helpers.validate_func_logs(args.binary)
    helpers.validate_paths(args)
    # Determine if cmplog should be used per task
    if args.use_cmplog_in_task is None:
        # If unspecified, enable only if cmplog_binary is set
        args.use_cmplog_in_task = bool(args.cmplog_binary)
        if args.use_cmplog_in_task:
            print("[*] Using cmplog within each task (default behavior, binary provided).")
    else:
        # If explicitly set
        if args.use_cmplog_in_task:
            if not args.cmplog_binary:
                print("[!] Ignoring --use_cmplog_in_task since no --cmplog_binary is provided.")
                args.use_cmplog_in_task = False
            else:
                print("[*] Using cmplog within each task as explicitly enabled.")
        else:
            if args.cmplog_binary:
                print("[*] --no-use_cmplog_in_task specified, cmplog binary will not be used in each task.")
    main(args.binary, args.afl_binary, args.profiling_binary, args.gcov_binary, args.gcov_folders,
         args.pre_arguments, args.post_arguments, args.seed_corpus, args.out_folder, args.dict,
         args.dot_file, func_ids, func_bbs, args.cores, args.algorithm, args.total_timeout,
         args.scanning_timeout, args.expl_timeout, args.target_folder, libafl_dir, args.cmplog_binary, args.use_cmplog_in_task)
